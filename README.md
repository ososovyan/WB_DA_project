# WB_DA_project
Проект буудет не ETL а ELT
начнем с цели
Исторический анализ данных WB

**Задачи**

# Написание клиента получения данных 
по api в общем виде(+ клиента для чтения из файлов) на баз одного родителя, для более модульного подключения разных источников

# Организация хранилища даннных 
- на базе облачной СУБД Posstgres
- создание настроек безопасности
- создание схем (слоев): 4 слоя
  - raw для сырых данных
  - proceed для обработанных
  - core основная схема данныз типа снежинка (нормализованная) для хранения данных
  - mart слой длдя хранения подготовлденных данных для аналитики, основа плоская (широкая, денорммализованная таблица) материальное представления для дальнейшего анализа и представления
- создание всех необходимых таблиц, индексов, сввязей
- *Создание дополнительных служебных таблиц для мониторинга
- *Создание чистильщика мусора через cron/Airflow
- ********Контрлоль миграций (это так наткнулся интересно почитать)
- !написать скрипт чтобы развернуть Архитектуру хранилища с нуля
- !Написание функций для пропуша данных вв след сллой
- Важная часть создание core слоя

# Клиент для подключения к бд 
- через SQlalchemy и декларированный подход
- подхотовить шаблоны таблиц

# Скрипты
- Скрипт для разввертывания хранилища данных
- Скрипт для загрузки всех основыынх даных для core слоя ии огранизации справочных таблиц
- Скрипт для загрузки показателей с проверкой наличия (с выбором опции выполнениия перезаписи основной таблицы, или дополенния для подгрузки недостающиих дат

# Аналитическая часть
- Подключене дашборда
- Аналлиз данных в python (немного переработать формат этап интерполяции унести в sql) в разведочном анализе


# Дополнительно 
- Проверка обновления данных по показателям за последние 15 лет (по отношщению к дате настоящего) подгрузка отсутстввующих дат и обновление показателей Airwlow (раз в месяц)
- Очистка промежуточных слоев raw (условно хранится 10 дней), proceed (2 дня) тоже через Airflow(может cron) (раз в день)

# **** Docker
